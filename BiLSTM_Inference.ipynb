{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For inference, please follow the instructions before the second last cell.\n"
      ],
      "metadata": {
        "id": "7n77GXCgodOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets torch numpy spacy joblib pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AIMTeEB0TfyV",
        "outputId": "a179d060-45bc-4c85-c807-1d62d6570217"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import joblib\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "import re\n",
        "import warnings\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "p5nJN_QWcooH"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "oI0lfR2Agh4K"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"sem_eval_2010_task_8\")"
      ],
      "metadata": {
        "id": "X7gbqFxvT_qv"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "special_tokens = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n",
        "\n",
        "def custom_tokenizer(nlp):\n",
        "    prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n",
        "    suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\n",
        "    infix_re = spacy.util.compile_infix_regex(nlp.Defaults.infixes)\n",
        "\n",
        "    tokenizer = Tokenizer(nlp.vocab,\n",
        "                          prefix_search=prefix_re.search,\n",
        "                          suffix_search=suffix_re.search,\n",
        "                          infix_finditer=infix_re.finditer,\n",
        "                          token_match=None)\n",
        "\n",
        "    for token in special_tokens:\n",
        "        tokenizer.add_special_case(token, [{\"ORTH\": token}])\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n",
        "\n",
        "def tokenize(text):\n",
        "    for token in special_tokens:\n",
        "        text = text.replace(token, f\" {token} \")  #Adding spaces to isolate markers\n",
        "\n",
        "    return [token.text.lower() for token in nlp(text)]"
      ],
      "metadata": {
        "id": "J2GAkpKOT7yG"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "\n",
        "def load_word2vec(word2idx):\n",
        "  word2vec = api.load(\"word2vec-google-news-300\")\n",
        "  vocab_size = len(word2idx)\n",
        "  embedding_matrix = np.random.uniform(-0.25, 0.25, (vocab_size, embedding_dim))\n",
        "\n",
        "  for word, idx in word2idx.items():\n",
        "      if word in word2vec:\n",
        "          embedding_matrix[idx] = word2vec[word]\n",
        "\n",
        "  return embedding_matrix"
      ],
      "metadata": {
        "id": "qp6zZ-nlQeOH"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_word_embeddings(word2idx, vecType):\n",
        "  if vecType == \"turian\":\n",
        "    embedding_file = \"embeddings-scaled.50.gz\"\n",
        "  elif  vecType == \"word2vec\":\n",
        "    word_embeddings = load_word2vec(word2idx)\n",
        "\n",
        "  return word_embeddings\n"
      ],
      "metadata": {
        "id": "9FV1MgWcQanu"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a mapping of vocabulary\n",
        "\n",
        "word_freq = Counter(word for sentence in dataset[\"train\"][\"sentence\"] for word in tokenize(sentence))\n",
        "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "\n",
        "for i, (word, _) in enumerate(word_freq.most_common(), start=2):\n",
        "    word2idx[word] = i"
      ],
      "metadata": {
        "id": "mwfh1rTaTia8"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Word2Vec embeddings are too large to share as a file (1.5GB). Please download. Will take ~5 min\n",
        "word_embeddings=load_word_embeddings(word2idx, \"word2vec\")"
      ],
      "metadata": {
        "id": "L_ZP1I3NQolG"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "C7zxA_iWP4Jc"
      },
      "outputs": [],
      "source": [
        "entity_pos = { \"inference\":[]}\n",
        "target_classes = 19\n",
        "num_clusters = 5\n",
        "batch_size = 32\n",
        "max_num_clusters=15\n",
        "max_sentence_len=80\n",
        "embedding_dim=300"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relation_names=[\n",
        "\"Cause-Effect(e1,e2)\",\n",
        "\"Cause-Effect(e2,e1)\",\n",
        "\"Component-Whole(e1,e2)\",\n",
        "\"Component-Whole(e2,e1)\",\n",
        "\"Content-Container(e1,e2)\",\n",
        "\"Content-Container(e2,e1)\",\n",
        "\"Entity-Destination(e1,e2)\",\n",
        "\"Entity-Destination(e2,e1)\",\n",
        "\"Entity-Origin(e1,e2)\",\n",
        "\"Entity-Origin(e2,e1)\",\n",
        "\"Instrument-Agency(e1,e2)\",\n",
        "\"Instrument-Agency(e2,e1)\",\n",
        "\"Member-Collection(e1,e2)\",\n",
        "\"Member-Collection(e2,e1)\",\n",
        "\"Message-Topic(e1,e2)\",\n",
        "\"Message-Topic(e2,e1)\",\n",
        "\"Product-Producer(e1,e2)\",\n",
        "\"Product-Producer(e2,e1)\",\n",
        "\"Other\"\n",
        "]"
      ],
      "metadata": {
        "id": "-KjS49A3QGtl"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encode_sentence(tokens, train_or_test, word2idx=word2idx, max_len=max_sentence_len):\n",
        "\n",
        "    #saving positions of entities in each sentence for later use\n",
        "    ei1_s = tokens.index(\"<e1>\")\n",
        "    ei1_e = tokens.index(\"</e1>\")\n",
        "    ei2_s = tokens.index(\"<e2>\")\n",
        "    ei2_e = tokens.index(\"</e2>\")\n",
        "    entity_pos[train_or_test].append(((ei1_s + 1, ei1_e),  (ei2_s + 1, ei2_e)))\n",
        "\n",
        "\n",
        "    #getting unique indices for each word based on vocab dictionary\n",
        "    indices = [word2idx.get(token, word2idx[\"<UNK>\"]) for token in tokens]\n",
        "    indices = indices[:max_len] + [word2idx[\"<PAD>\"]] * (max_len - len(indices))\n",
        "\n",
        "    return torch.tensor(indices, dtype=torch.long)"
      ],
      "metadata": {
        "id": "XNYnI_05Srxx"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loader(data, train_or_test, shuffle, entity_hidden_states = None, entity_features = None):\n",
        "  encoded_sentences = [encode_sentence(tokenize(sentence), train_or_test) for sentence in data[\"sentence\"]]\n",
        "  labels = torch.tensor(data[\"relation\"], dtype=torch.long)\n",
        "  sentences_tensor = torch.stack(encoded_sentences)\n",
        "\n",
        "  dataset_args = [sentences_tensor, labels]\n",
        "\n",
        "  if entity_hidden_states is not None and entity_features is not None:\n",
        "    dataset_args.append(entity_hidden_states)\n",
        "    dataset_args.append(entity_features)\n",
        "\n",
        "  torch_dataset = TensorDataset(*dataset_args)\n",
        "  dataloader = DataLoader(torch_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "wLzUzhkHSnfx"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hs_mean(sent_hidden_states, ep):\n",
        "  start = ep[0]\n",
        "  end = ep[1]\n",
        "  if start == end:\n",
        "    return sent_hidden_states[start]\n",
        "  else:\n",
        "    return torch.sum(sent_hidden_states[start:end], dim=0) / (end - start)"
      ],
      "metadata": {
        "id": "YcZdAAMKVVK7"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entities_hs_from_sentence(lstm_out, pos_index, train_or_test):\n",
        "\n",
        "  en_hs_list = []\n",
        "  for sent_hid_states in (lstm_out):\n",
        "    (e1p, e2p) = entity_pos[train_or_test][pos_index]\n",
        "    h1 = get_hs_mean(sent_hid_states, e1p)\n",
        "    h2 = get_hs_mean(sent_hid_states, e2p)\n",
        "    en_pair = torch.stack([h1, h2]) #dim => 2 x hidden_dim * 2\n",
        "    en_hs_list.append(en_pair)\n",
        "    pos_index += 1\n",
        "\n",
        "\n",
        "  return torch.stack(en_hs_list), pos_index #dim => batch_size x 2 x hidden_dim * 2"
      ],
      "metadata": {
        "id": "bpGZ2GSmVTKh"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entity_hidden_states(model, loader, train_or_test):\n",
        "\n",
        "  #Getting the hidden states of entities based on position in sentence\n",
        "\n",
        "  model.eval()\n",
        "  en_hs_tensors_list = []\n",
        "  with torch.no_grad():\n",
        "      pos_index = 0\n",
        "      for x_batch, y_batch in loader:\n",
        "        _, lstm_out = model(x_batch)\n",
        "        en_hs_list, pos_index = get_entities_hs_from_sentence(lstm_out, pos_index, train_or_test)\n",
        "        en_hs_tensors_list.append(en_hs_list)\n",
        "\n",
        "      return torch.cat(en_hs_tensors_list, dim=0)  #dim => batch_size, 2, hidden_dim * 2\n"
      ],
      "metadata": {
        "id": "5QcZo909VQrR"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entity_type_features(hidden_states, clustering_type, model, hs_array=None):\n",
        "\n",
        "  if hs_array is None:\n",
        "    hs_array = np.array([e for tup in hidden_states for e in tup])\n",
        "\n",
        "  if clustering_type == 'KMeans':\n",
        "    entity_types = model.predict(hs_array)\n",
        "  elif clustering_type == 'GaussianMixture':\n",
        "    soft_labels = model.predict_proba(hs_array)\n",
        "    component_indices = np.arange(num_clusters)\n",
        "    entity_types = np.dot(soft_labels, component_indices)  # (number of entities,) weighted assignment\n",
        "\n",
        "\n",
        "  entity_type_embeddings = nn.Embedding(num_clusters, embedding_dim=hidden_dim*2)\n",
        "  entity_type_features = entity_type_embeddings(torch.tensor(entity_types, dtype=torch.long)) #shape = (num_of_entities, hidden_dim * 2)\n",
        "\n",
        "  num_sentences = entity_type_features.shape[0] // 2\n",
        "  entity_type_features = entity_type_features.view(num_sentences, 2, -1)  #dim => num_setences, num_entities per sentence (=2), hidden_dim * 2\n",
        "\n",
        "  return entity_type_features\n"
      ],
      "metadata": {
        "id": "AthFE62sZLNx"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RE_BiLSTM(nn.Module):\n",
        "  def __init__(self, embeddings, hidden_dim, vocab_size, num_layers, dropout_rate, output_dim=target_classes, embedding_dim=embedding_dim):\n",
        "    super(RE_BiLSTM, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2idx['<PAD>'])\n",
        "    if embeddings is not None:\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embeddings))\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
        "    self.embedding_dropout = nn.Dropout(dropout_rate)\n",
        "    self.output_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
        "    embedded = self.embedding_dropout(embedded)\n",
        "    lstm_out, _ = self.lstm(embedded)  # [batch_size, seq_len, hidden_dim*2]\n",
        "    lstm_out = self.output_dropout(lstm_out)\n",
        "\n",
        "    return lstm_out"
      ],
      "metadata": {
        "id": "-QRl_VEEWNMH"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityAwareAttention(nn.Module):\n",
        "  def __init__(self, hidden_dim, dropout_rate, w_norm_threshold=0.5):\n",
        "    super(EntityAwareAttention, self).__init__()\n",
        "    self.tanh_hs = nn.Tanh()\n",
        "    self.w = nn.Parameter(torch.randn(hidden_dim * 2))\n",
        "    self.tanh_sent = nn.Tanh()\n",
        "    self.linear = nn.Linear(hidden_dim * 2, target_classes)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.w_norm_threshold=w_norm_threshold\n",
        "\n",
        "  def forward(self, hidden_states):\n",
        "    #hidden_states dim => batch_size, T, d\n",
        "    M = self.tanh_hs(hidden_states) #word_features dim => batch_size x T x d, where d = 2 * hidden_dim and T= max sentence length\n",
        "    alpha = torch.softmax(torch.matmul(M, self.w), dim=1)  #attention scores, batch_size x T dimensional\n",
        "    alpha = alpha.unsqueeze(1)  # Shape: [batch_size, 1, T]\n",
        "    r = torch.bmm(alpha, hidden_states).squeeze(1)  #batch_size x d dimensional\n",
        "    h_star = self.tanh_sent(r) #batch_size x d dimensional\n",
        "    h_star = self.dropout(h_star)\n",
        "    preds = self.linear(h_star) #batch_size x target_classes\n",
        "\n",
        "    return preds\n",
        "\n"
      ],
      "metadata": {
        "id": "zLGL4RFVWsxi"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RE_Model(nn.Module):\n",
        "  def __init__(self, embeddings, hidden_dim, vocab_size, lstm_layers,\n",
        "               dropout_rate, output_dim=target_classes, embedding_dim=embedding_dim,  use_entity_attention=True):\n",
        "    super(RE_Model, self).__init__()\n",
        "    self.lstm = RE_BiLSTM(embeddings, hidden_dim, vocab_size, num_layers=lstm_layers, dropout_rate=dropout_rate)\n",
        "    self.WH = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
        "    self.WE = nn.Linear((hidden_dim * 2) * 4, hidden_dim * 2)\n",
        "    self.ent_att = EntityAwareAttention(hidden_dim, dropout_rate)\n",
        "    self.use_entity_attention = use_entity_attention\n",
        "    self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x, entity_hidden_states, entity_types):\n",
        "    lstm_out = self.lstm(x)\n",
        "\n",
        "    if self.use_entity_attention:\n",
        "      entity_hidden_states = torch.cat((entity_hidden_states[:, 0, :], entity_hidden_states[:, 1, :]), dim=-1)\n",
        "      entity_types = torch.cat((entity_types[:, 0, :], entity_types[:, 1, :]), dim=-1)\n",
        "      entity_hidden_states.unsqueeze(1)\n",
        "      entity_types.unsqueeze(1)\n",
        "\n",
        "      #dim of both tensors above => batch_size, 1, (hidden_dim * 2) * 2\n",
        "\n",
        "      entity_features = torch.cat([entity_hidden_states, entity_types], dim = -1) #dim =>[batch_size, 1, (hidden_dim * 2) * 2]\n",
        "\n",
        "      entity_features = self.dropout(entity_features)\n",
        "      e = self.layer_norm(self.WE(entity_features))\n",
        "      l = self.WH(lstm_out)\n",
        "      e = e.unsqueeze(1)\n",
        "\n",
        "      word_features = e + l\n",
        "      preds = self.ent_att(word_features)\n",
        "    else:\n",
        "      preds = self.ent_att(lstm_out)\n",
        "\n",
        "\n",
        "    return preds\n"
      ],
      "metadata": {
        "id": "pA3SVEC8Wto6"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityEncoderBiLSTM(nn.Module):\n",
        "  def __init__(self, embeddings, hidden_dim, vocab_size, num_layers, dropout_rate, output_dim=target_classes, embedding_dim=embedding_dim):\n",
        "    super(EntityEncoderBiLSTM, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2idx['<PAD>'])\n",
        "    if embeddings is not None:\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embeddings))\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
        "    self.embedding_dropout = nn.Dropout(dropout_rate)\n",
        "    self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.lstm_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    embedded = self.embedding_dropout(embedded)\n",
        "    lstm_out, _ = self.lstm(embedded)\n",
        "    lstm_out = self.lstm_dropout(lstm_out)\n",
        "    pooled = torch.mean(lstm_out, dim=1)\n",
        "    output = self.dropout(pooled)\n",
        "    output = self.fc(output)\n",
        "    return output, lstm_out\n",
        "\n"
      ],
      "metadata": {
        "id": "D17N_RyVWwP2"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 256\n",
        "dropout_rate = 0.44586702274269596\n",
        "num_layers = 2\n",
        "weight_decay=1e-05\n",
        "epochs= 25\n",
        "learning_rate=1\n",
        "\n",
        "\n",
        "entity_lstm_model = EntityEncoderBiLSTM(embeddings=word_embeddings, hidden_dim=hidden_dim, vocab_size=len(word2idx), num_layers=num_layers,\n",
        "                       dropout_rate=dropout_rate, output_dim=target_classes)\n",
        "entity_lstm_model.load_state_dict(torch.load(\"encoder_weights.pth\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLuvdUB3VZ1E",
        "outputId": "985050e5-215b-4eda-b263-0bd2159f9c8f"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 256\n",
        "dropout_rate = 0.44586702274269596\n",
        "learning_rate = 1\n",
        "num_layers = 1\n",
        "weight_decay= 1e-05\n",
        "epochs= 15\n",
        "re_model_wo_types = RE_Model(embeddings=word_embeddings, hidden_dim=hidden_dim, vocab_size=len(word2idx), lstm_layers=num_layers,\n",
        "                      dropout_rate=dropout_rate, use_entity_attention=False)\n",
        "re_model_wo_types.load_state_dict(torch.load(\"RE_without_entities_model_weights.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fePRE5kg6oT",
        "outputId": "f0efebf0-71e4-4b0a-8151-ef8924eef32c"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = joblib.load(\"kmeans_model.pkl\")\n",
        "gmm = joblib.load(\"gmm_model.pkl\")"
      ],
      "metadata": {
        "id": "9L7B_9zkbbek"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 256\n",
        "dropout_rate = 0.55\n",
        "learning_rate = 1\n",
        "num_layers = 1\n",
        "weight_decay= 1e-02\n",
        "epochs= 15\n",
        "\n",
        "re_model_kmeans = RE_Model(embeddings=word_embeddings, hidden_dim=hidden_dim, vocab_size=len(word2idx), lstm_layers=num_layers,\n",
        "                      dropout_rate=dropout_rate, use_entity_attention=True)\n",
        "re_model_kmeans.load_state_dict(torch.load(\"RE_with_kmeans_entities_model_weights.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0WRhi8Hbz8R",
        "outputId": "3c96b777-d59c-43bd-a6df-0c745f0f2b05"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 256\n",
        "dropout_rate = 0.55\n",
        "learning_rate = 1\n",
        "num_layers = 1\n",
        "weight_decay= 1e-02\n",
        "epochs= 15\n",
        "\n",
        "re_model_gmm = RE_Model(embeddings=word_embeddings, hidden_dim=hidden_dim, vocab_size=len(word2idx), lstm_layers=num_layers,\n",
        "                      dropout_rate=dropout_rate, use_entity_attention=True)\n",
        "re_model_gmm.load_state_dict(torch.load(\"RE_with_gmm_entities_model_weights.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYgoXMDxgtgS",
        "outputId": "59025ece-501d-475f-8c82-d2c20cbef7f8"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(model, sentences_tensor, entity_hidden_states, infer_entity_type_features):\n",
        "  if (entity_hidden_states is None) and (infer_entity_type_features is None):\n",
        "    preds, _ = model(sentences_tensor)\n",
        "  else:\n",
        "    preds = model(sentences_tensor, infer_entity_hidden_states, infer_entity_type_features)\n",
        "\n",
        "  probs = torch.softmax(preds, dim=1)\n",
        "  max_indices = torch.tensor(torch.argmax(probs, dim=1))\n",
        "  predicted_relations = [relation_names[idx] for idx in max_indices.tolist()]\n",
        "\n",
        "  combined_array = np.column_stack((max_indices.numpy(), np.array(predicted_relations, dtype=object)))\n",
        "\n",
        "  return combined_array"
      ],
      "metadata": {
        "id": "nExPAAY5hTxX"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please enter a list of sentences in the cell below.\n",
        "\n",
        "\n",
        "1.   The max length of the sentence is 80 characters.\n",
        "2.   e1 and e2 should be marked as tags e.g. <e1>...</e1>\n",
        "3.   We assume only one pair of entities per sentence\n",
        "\n",
        "Then run the last cell. The code will display a Pandas Dataframe with the prediction for each sentence based on all 4 experimental LSTM models we developed"
      ],
      "metadata": {
        "id": "-hwSOmyVmrzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences =  [ \"<e1>John</e1> works at <e2>Google</e2>.\",  \"The<e1>storm</e1> caused poor <e2>visibility</e2>.\"]\n",
        "#e.g. sentence = \"<e1>John</e1> works at <e2>Google</e2>.\""
      ],
      "metadata": {
        "id": "ywQSp5GSnRRN"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_sentences = [{\"sentence\": sentence, \"relation\":-1} for sentence in sentences]\n",
        "\n",
        "inference_dataset = Dataset.from_list(inference_sentences)\n",
        "infer_loader = get_data_loader(inference_dataset, \"inference\", False)\n",
        "infer_entity_hidden_states = get_entity_hidden_states(entity_lstm_model, infer_loader, \"inference\")\n",
        "infer_entity_type_features = get_entity_type_features(infer_entity_hidden_states, \"KMeans\", kmeans)\n",
        "encoded_sentences = [torch.tensor(encode_sentence(x, \"inference\"), dtype=torch.long) for x in inference_dataset[\"sentence\"]]\n",
        "sentences_tensor = torch.stack(encoded_sentences)\n",
        "\n",
        "data1 = get_prediction(entity_lstm_model, sentences_tensor, None, None)\n",
        "df1 = pd.DataFrame(data1, columns=[\"Relation Number\", \"Relation Name\"])\n",
        "df1[\"Model Name\"]=\"BiLSTM\"\n",
        "df1 = df1[[\"Model Name\", \"Relation Number\", \"Relation Name\"]]\n",
        "\n",
        "\n",
        "data2 = get_prediction(re_model_wo_types, sentences_tensor, infer_entity_hidden_states, infer_entity_type_features)\n",
        "df2 = pd.DataFrame(data2, columns=[\"Relation Number\", \"Relation Name\"])\n",
        "df2[\"Model Name\"]=\"Att-LSTM (w/o Entity Types)\"\n",
        "df2 = df2[[\"Model Name\", \"Relation Number\", \"Relation Name\"]]\n",
        "\n",
        "data3 = get_prediction(re_model_kmeans, sentences_tensor, infer_entity_hidden_states, infer_entity_type_features)\n",
        "df3 = pd.DataFrame(data3, columns=[\"Relation Number\", \"Relation Name\"])\n",
        "df3[\"Model Name\"]=\"Att-LSTM (KMeans)\"\n",
        "df3 = df3[[\"Model Name\", \"Relation Number\", \"Relation Name\"]]\n",
        "\n",
        "data4 = get_prediction(re_model_gmm, sentences_tensor, infer_entity_hidden_states, infer_entity_type_features)\n",
        "df4 = pd.DataFrame(data4, columns=[\"Relation Number\", \"Relation Name\"])\n",
        "df4[\"Model Name\"]=\"Att-LSTM (GMMs)\"\n",
        "df4 = df4[[\"Model Name\", \"Relation Number\", \"Relation Name\"]]\n",
        "\n",
        "result = pd.concat([df1, df2, df3, df4], axis=0)\n",
        "\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "t4wD3mjlQNfN",
        "outputId": "24a5ebbf-1daa-47f7-e866-31f38129b525"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Model Name Relation Number              Relation Name\n",
              "0                       BiLSTM               6  Entity-Destination(e1,e2)\n",
              "1                       BiLSTM               6  Entity-Destination(e1,e2)\n",
              "0  Att-LSTM (w/o Entity Types)              18                      Other\n",
              "1  Att-LSTM (w/o Entity Types)              18                      Other\n",
              "0            Att-LSTM (KMeans)              18                      Other\n",
              "1            Att-LSTM (KMeans)               0        Cause-Effect(e1,e2)\n",
              "0              Att-LSTM (GMMs)              18                      Other\n",
              "1              Att-LSTM (GMMs)               0        Cause-Effect(e1,e2)"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d66af17b-6604-4377-8dd8-0e03bcf7e416\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Name</th>\n",
              "      <th>Relation Number</th>\n",
              "      <th>Relation Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BiLSTM</td>\n",
              "      <td>6</td>\n",
              "      <td>Entity-Destination(e1,e2)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BiLSTM</td>\n",
              "      <td>6</td>\n",
              "      <td>Entity-Destination(e1,e2)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Att-LSTM (w/o Entity Types)</td>\n",
              "      <td>18</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Att-LSTM (w/o Entity Types)</td>\n",
              "      <td>18</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Att-LSTM (KMeans)</td>\n",
              "      <td>18</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Att-LSTM (KMeans)</td>\n",
              "      <td>0</td>\n",
              "      <td>Cause-Effect(e1,e2)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Att-LSTM (GMMs)</td>\n",
              "      <td>18</td>\n",
              "      <td>Other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Att-LSTM (GMMs)</td>\n",
              "      <td>0</td>\n",
              "      <td>Cause-Effect(e1,e2)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d66af17b-6604-4377-8dd8-0e03bcf7e416')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d66af17b-6604-4377-8dd8-0e03bcf7e416 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d66af17b-6604-4377-8dd8-0e03bcf7e416');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-03920bc5-9a75-4ae6-8e92-75f2c003f301\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-03920bc5-9a75-4ae6-8e92-75f2c003f301')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-03920bc5-9a75-4ae6-8e92-75f2c003f301 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_99de7fc7-1381-4775-a79e-4649bfbe72ef\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('result')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_99de7fc7-1381-4775-a79e-4649bfbe72ef button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('result');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "result",
              "summary": "{\n  \"name\": \"result\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Model Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Att-LSTM (w/o Entity Types)\",\n          \"Att-LSTM (GMMs)\",\n          \"BiLSTM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Relation Number\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 18,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          6,\n          18,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Relation Name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Entity-Destination(e1,e2)\",\n          \"Other\",\n          \"Cause-Effect(e1,e2)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    }
  ]
}