{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsiEnRdnNuJT",
        "outputId": "ed769dff-e9b4-40e1-98db-2bd89b1f79a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KZWUDrCjNB1d"
      },
      "outputs": [],
      "source": [
        "questions = {\n",
        "    \"Cause-Effect(e1,e2)\": [\"What is caused by <e1>?\", \"What causes <e2>?\"],\n",
        "    \"Cause-Effect(e2,e1)\": [\"What causes <e1>?\", \"What is caused by <e2>?\"],\n",
        "    \"Instrument-Agency(e1,e2)\": [\"What uses <e1>?\", \"What is the instrument of <e2>?\"],\n",
        "    \"Instrument-Agency(e2,e1)\": [\"What is the instrument of <e1>?\", \"What uses <e2>?\"],\n",
        "    \"Product-Producer(e1,e2)\": [\"What produces <e1>?\", \"What is the product of <e2>?\"],\n",
        "    \"Product-Producer(e2,e1)\": [\"What is the product of <e1>?\", \"What produces <e2>?\"],\n",
        "    \"Content-Container(e1,e2)\": [\"What stores <e1>?\", \"What is stored inside <e2>?\"],\n",
        "    \"Content-Container(e2,e1)\": [\"What is stored inside <e1>?\", \"What stores <e2>?\"],\n",
        "    \"Entity-Origin(e1,e2)\": [\"What is the origin of <e1>?\", \"What entity originates from <e2>?\"],\n",
        "    \"Entity-Origin(e2,e1)\": [\"What entity originates from <e1>?\", \"What is the origin of <e2>?\"],\n",
        "    \"Entity-Destination(e1,e2)\": [\"What is the destination of <e1>?\", \"What entity is moving toward <e2>?\"],\n",
        "    \"Entity-Destination(e2,e1)\": [\"What entity is moving toward <e1>?\", \"What is the destination of <e2>?\"],\n",
        "    \"Component-Whole(e1,e2)\": [\"What entity does <e1> operate within?\", \"What functional component of <e2> is mentioned?\"],\n",
        "    \"Component-Whole(e2,e1)\": [\"What functional component of <e1> is mentioned?\", \"What entity does <e2> operate within?\"],\n",
        "    \"Member-Collection(e1,e2)\": [\"Which collection/organization does <e1> belong to?\", \"Who are the members of <e2>?\"],\n",
        "    \"Member-Collection(e2,e1)\": [\"Who are the members of <e1>?\", \"Which collection/organization does <e2> belong to?\"],\n",
        "    \"Message-Topic(e1,e2)\": [\"What is the main topic discussed in <e1>?\", \"Which message contains information about <e2>?\"],\n",
        "    \"Message-Topic(e2,e1)\": [\"Which message contains information about <e1>?\", \"What is the main topic discussed in <e2>?\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "91fVyv0yOIuV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_name = \"huawei-noah/TinyBERT_General_6L_768D\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tinybert = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "class TinyBERTForQA(torch.nn.Module):\n",
        "    def __init__(self, tinybert):\n",
        "        super().__init__()\n",
        "        self.tinybert = tinybert\n",
        "        self.qa_outputs = torch.nn.Linear(768, 2)  # Add QA head for start and end logits\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        outputs = self.tinybert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        logits = self.qa_outputs(outputs.last_hidden_state)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)  # Split into start and end logits\n",
        "        return start_logits.squeeze(-1), end_logits.squeeze(-1)\n",
        "\n",
        "# Initialise new model\n",
        "model = TinyBERTForQA(tinybert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj8HpJVCdTKg"
      },
      "source": [
        "- Change the directory to load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onuaUk53OSd-",
        "outputId": "6db9168e-6566-45fb-fd8e-a749f51584bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-4a442bef92f2>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(directory))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TinyBERTForQA(\n",
              "  (tinybert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "directory = \"/content/drive/My Drive/relation_extraction_model_final.pth\"\n",
        "model.load_state_dict(torch.load(directory))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB3FHBz7QMP-",
        "outputId": "46bfd0ee-c003-47cb-8906-c92a3dc759ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TinyBERTForQA(\n",
              "  (tinybert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "j6fyDwy9NlzO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def inference(sentence):\n",
        "    clean_sentence = re.sub(r\"</?e[12]>\", \" \", sentence)\n",
        "\n",
        "    # Extract e1 and e2\n",
        "    e1_match = re.search(r\"<e1>(.*?)</e1>\", sentence)\n",
        "    e2_match = re.search(r\"<e2>(.*?)</e2>\", sentence)\n",
        "    entities = {\n",
        "        'e1': e1_match.group(1) if e1_match else None,\n",
        "        'e2': e2_match.group(1) if e2_match else None\n",
        "    }\n",
        "    answer = []\n",
        "    for rel, q_list in questions.items(): # Loop for each relation\n",
        "        # first_entity, second_entity = rel[-7:].replace('(', '').replace(')', '').split(',')\n",
        "        switch = False\n",
        "        for q in q_list: # Loop for each question\n",
        "            question = q.replace('<e1>', entities['e1']).replace('<e2>', entities['e2']) # Sub the entity to the question\n",
        "\n",
        "            if switch:\n",
        "                true_answer = entities['e1']\n",
        "            else:\n",
        "                true_answer = entities['e2']\n",
        "            switch = not switch\n",
        "\n",
        "            # Tokenizer the input\n",
        "            inputs = tokenizer(\n",
        "                clean_sentence,\n",
        "                question,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=80,\n",
        "                padding=\"max_length\",\n",
        "                return_offsets_mapping=True\n",
        "            )\n",
        "\n",
        "            # Prepare to convert the start and end token position back to word\n",
        "            tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze())\n",
        "            offsets = inputs[\"offset_mapping\"].squeeze()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=inputs[\"input_ids\"].to(device),\n",
        "                    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                    token_type_ids=inputs[\"token_type_ids\"].to(device)\n",
        "                )\n",
        "\n",
        "                start_logits, end_logits = outputs # start and end position matrix\n",
        "                pred_start = torch.argmax(start_logits, dim=1).item() # Choose the highest value start pos\n",
        "                pred_end = torch.argmax(end_logits, dim=1).item() # Choose the highest value end pos\n",
        "\n",
        "                # pred_end is not before pred_start\n",
        "                if pred_start <= pred_end:\n",
        "                    start_char = offsets[pred_start][0].item()\n",
        "                    end_char = offsets[pred_end][1].item()\n",
        "\n",
        "                    predicted_word = clean_sentence[start_char:end_char]\n",
        "                    # print(predicted_word)\n",
        "                    answer.append((rel, question, predicted_word, true_answer))\n",
        "\n",
        "\n",
        "    # print('Length of answer list: ', len(answer))\n",
        "    # print(answer)\n",
        "    for i in range(0, len(answer), 2):\n",
        "        relation = answer[i][0]\n",
        "        if answer[i][2] != '' and answer[i+1][2] != '':\n",
        "            if answer[i][2] == answer[i][3] or answer[i+1][2] == answer[i+1][3]:\n",
        "                print(answer[i][1:3])\n",
        "                print(answer[i+1][1:3])\n",
        "                return relation\n",
        "    return 'Other'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbzL_bGKdHde"
      },
      "source": [
        "- INPUT THE SENTENCE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0dehnucBNpZd"
      },
      "outputs": [],
      "source": [
        "sentences = [\"The most common <e1>audits</e1> were about <e2>waste</e2> and recycling.\",\n",
        "             \"The <e1>computer</e1> is kept in a common <e2>area</e2> within our home.\",\n",
        "             \"Broken <e1>bones</e1> (also called fractures) in the <e2>foot</e2> are very common.\",\n",
        "             \"The <e1>company</e1> has mocked up a <e2>version</e2> of YouTube built around the HTML5 video tag, playing mini-movies inside a browser sans plug-ins.\",\n",
        "             \"After handsome renovations at various locales, the <e1>company</e1> has remodeled a church into a <e2>home</e2>.\",\n",
        "             \"This <e1>football match</e1> is <e2>interesting</e2>.\",\n",
        "             \"The two <e1>countries</e1> are related through an unbroken common <e2>history</e2> spanning three-hundred and thirty-nine years.\",\n",
        "             \"Basic diagrams also work well on the <e1>computer</e1> <e2>screen</e2> if they are carefully designed to match the grid of pixels on the screen.\",\n",
        "             \"The city of Chicago lost 725000 residents between 1950 and 2000, yet 82 percent of the suburban <e1>growth</e1> was from outside the metropolitan <e2>area</e2>.\",\n",
        "             \"While making <e1>observations</e1> the microfossil through the binocular microscope or on a computer <e2>monitor</e2>, the investigator needed to manually move the specimen.\",\n",
        "             \"The <e1>storm</e1> was generated by an intense <e2>cold front</e2> moving across drought-affected areas in South Australia and NSW.\",\n",
        "             \"<e1>Fainting</e1> is a common cause of <e2>unconsciousness</e2> and may occur when the casualty's heart rate is too slow to maintain sufficient blood pressure for the brain.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC1xrz_ROk5T",
        "outputId": "c837430a-82e4-4737-918e-2689513f3e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('What is the main topic discussed in audits?', 'waste')\n",
            "('Which message contains information about waste?', 'audits')\n",
            "The relation of the sentence is:  Message-Topic(e1,e2)\n",
            "----------------------------------------------------------\n",
            "The relation of the sentence is:  Other\n",
            "----------------------------------------------------------\n",
            "('What entity does bones operate within?', 'foot')\n",
            "('What functional component of foot is mentioned?', 'bones')\n",
            "The relation of the sentence is:  Component-Whole(e1,e2)\n",
            "----------------------------------------------------------\n",
            "The relation of the sentence is:  Other\n",
            "----------------------------------------------------------\n",
            "The relation of the sentence is:  Other\n",
            "----------------------------------------------------------\n",
            "The relation of the sentence is:  Other\n",
            "----------------------------------------------------------\n",
            "('Which message contains information about countries?', 'history')\n",
            "('What is the main topic discussed in history?', 'The two  countries')\n",
            "The relation of the sentence is:  Message-Topic(e2,e1)\n",
            "----------------------------------------------------------\n",
            "('What entity does computer operate within?', 'screen')\n",
            "('What functional component of screen is mentioned?', 'grid')\n",
            "The relation of the sentence is:  Component-Whole(e1,e2)\n",
            "----------------------------------------------------------\n",
            "('What is the origin of growth?', 'The city of Chicago lost 725000 residents between 1950 and 2000, yet 82 percent of the suburban  growth  was from outside the metropolitan  area')\n",
            "('What entity originates from area?', 'growth')\n",
            "The relation of the sentence is:  Entity-Origin(e1,e2)\n",
            "----------------------------------------------------------\n",
            "The relation of the sentence is:  Other\n",
            "----------------------------------------------------------\n",
            "('What causes storm?', 'cold front')\n",
            "('What is caused by cold front?', 'storm')\n",
            "The relation of the sentence is:  Cause-Effect(e2,e1)\n",
            "----------------------------------------------------------\n",
            "('What is caused by Fainting?', 'unconsciousness')\n",
            "('What causes unconsciousness?', 'Fainting')\n",
            "The relation of the sentence is:  Cause-Effect(e1,e2)\n",
            "----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for sentence in sentences:\n",
        "    print('The relation of the sentence is: ', inference(sentence))\n",
        "    print('----------------------------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
